{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json as js\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "\n",
    "base_dir = \"C:\\\\Users\\\\1\\\\Desktop\\\\datasets\"\n",
    "data_buffer = []\n",
    "max_data = 2000\n",
    "\n",
    "for json_file in os.listdir(base_dir)[:-1]:\n",
    "\n",
    "\n",
    "    curent_file = os.path.join(base_dir, json_file)\n",
    "    curent_data_buffer = {}\n",
    "    with open (curent_file) as file:\n",
    "\n",
    "        json_data = file.readlines()\n",
    "\n",
    "        for (json_number, json_per_line) in enumerate(json_data):\n",
    "            \n",
    "            if json_number == max_data:\n",
    "                \n",
    "                break\n",
    "\n",
    "            json_format = js.loads(json_per_line)\n",
    "            curent_data_buffer[f\"subject_number: {json_number}\"] = json_format\n",
    "            max_data += 1\n",
    "    \n",
    "    data_buffer.append(curent_data_buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.6906149e+07 7.5000000e+03 1.0000000e+01 ... 1.3050000e+03\n",
      "  1.0190000e+04 0.0000000e+00]\n",
      " [1.6906158e+07 7.5000000e+03 1.3000000e+01 ... 1.3050000e+03\n",
      "  1.0190000e+04 0.0000000e+00]\n",
      " [1.6906167e+07 7.5000000e+03 7.0000000e+00 ... 1.3050000e+03\n",
      "  1.0190000e+04 0.0000000e+00]\n",
      " ...\n",
      " [1.7056539e+07 7.5000000e+03 1.8400000e+02 ... 1.9741000e+04\n",
      "  9.4800000e+02 1.0000000e+00]\n",
      " [1.7056557e+07 7.5000000e+03 1.8400000e+02 ... 1.9741000e+04\n",
      "  9.4800000e+02 1.0000000e+00]\n",
      " [1.7056764e+07 7.5000000e+03 1.4700000e+02 ... 1.9741000e+04\n",
      "  9.4800000e+02 1.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "samples_data_tensor = []\n",
    "for (batch_number, data_batch) in enumerate(data_buffer):\n",
    "\n",
    "    for (subject_number, subject) in enumerate(data_batch):\n",
    "\n",
    "        try:\n",
    "\n",
    "            for sample in data_batch[subject][\"steps\"][\"samples\"]:\n",
    "\n",
    "\n",
    "                sample_vector = np.asarray([float(feature) for feature in sample.values()])\n",
    "                sample_vector[0] *= (10 ** -5)\n",
    "                sample_vector[1] /= 120.0\n",
    "\n",
    "                if batch_number == 0:\n",
    "\n",
    "                    add_vector = np.asarray([\n",
    "                        float(data_batch[subject][\"steps\"][\"steps\"]),\n",
    "                        float(data_batch[subject][\"steps\"][\"day\"]),\n",
    "                        float(data_batch[subject][\"steps\"][\"meters\"]),\n",
    "                        0\n",
    "                    ])\n",
    "                    \n",
    "                else:\n",
    "\n",
    "                    add_vector = np.asarray([\n",
    "                        float(data_batch[subject][\"steps\"][\"steps\"]),\n",
    "                        float(data_batch[subject][\"steps\"][\"day\"]),\n",
    "                        float(data_batch[subject][\"steps\"][\"meters\"]),\n",
    "                        1\n",
    "                    ])\n",
    "                    \n",
    "                result_features_vector = np.concatenate((sample_vector, add_vector))\n",
    "                samples_data_tensor.append(result_features_vector)\n",
    "        \n",
    "        except BaseException:\n",
    "\n",
    "            pass\n",
    "            \n",
    "\n",
    "\n",
    "samples_data_tensor = np.asarray(samples_data_tensor)\n",
    "print(samples_data_tensor)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train tensor shape: (17656911, 6), \n",
      " Test tensor shape: (17656912, 6)\n",
      " \n",
      "Data example: [[1.6963218e+07 7.5000000e+03 2.2200000e+02 ... 1.9633000e+04\n",
      "  1.1570000e+04 1.0000000e+00]\n",
      " [1.6879635e+07 7.5000000e+03 1.6370000e+03 ... 1.2740000e+03\n",
      "  5.8660000e+03 1.0000000e+00]\n",
      " [1.6996437e+07 7.5000000e+03 2.6000000e+01 ... 1.9671000e+04\n",
      "  2.0680000e+03 1.0000000e+00]\n",
      " ...\n",
      " [1.6936461e+07 7.5000000e+03 2.4600000e+02 ... 1.9602000e+04\n",
      "  1.7397000e+04 1.0000000e+00]\n",
      " [1.6952220e+07 7.5000000e+03 3.3000000e+02 ... 1.9620000e+04\n",
      "  5.9390000e+03 1.0000000e+00]\n",
      " [1.6883937e+07 7.5000000e+03 1.9000000e+01 ... 1.2790000e+03\n",
      "  1.5169000e+04 1.0000000e+00]] \n",
      " Labels example: \t(17656912,)\n",
      "0 label was found!!!\n"
     ]
    }
   ],
   "source": [
    "permutated_data_tensor = np.random.permutation(samples_data_tensor)\n",
    "\n",
    "train_data = permutated_data_tensor[:samples_data_tensor.shape[0] // 2, :-1]\n",
    "train_labels = permutated_data_tensor[:samples_data_tensor.shape[0] // 2, -1]\n",
    "\n",
    "validation_data = permutated_data_tensor[samples_data_tensor.shape[0] // 2:, :-1]\n",
    "validation_labels = permutated_data_tensor[samples_data_tensor.shape[0] // 2:, -1]\n",
    "\n",
    "print(f\"Train tensor shape: {train_data.shape}, \\n Test tensor shape: {validation_data.shape}\")\n",
    "print(f\" \\nData example: {permutated_data_tensor} \\n Labels example: \\t{validation_labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = tf.keras.Input(shape=(6, ))\n",
    "\n",
    "layer = tf.keras.layers.Dense(100, activation=\"relu\")(input_tensor)\n",
    "layer = tf.keras.layers.Dense(64, activation=\"relu\")(layer)\n",
    "layer = tf.keras.layers.Dense(64, activation=\"relu\")(layer)\n",
    "layer = tf.keras.layers.Dense(32, activation=\"relu\")(layer)\n",
    "layer = tf.keras.layers.Dense(32, activation=\"relu\")(layer)\n",
    "layer = tf.keras.layers.Dense(10, activation=\"relu\")(layer)\n",
    "last_layer = tf.keras.layers.Dense(1, activation=\"sigmoid\")(layer)\n",
    "\n",
    "model = tf.keras.Model(input_tensor, last_layer)\n",
    "model.compile(\n",
    "    optimizer=tf.optimizers.RMSprop(learning_rate=0.01),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=tf.keras.metrics.Accuracy()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_history = model.fit(train_data, train_labels,\n",
    "                          epochs=100,\n",
    "                          batch_size=30,\n",
    "                          validation_data=(validation_data, validation_labels))\n",
    "model.save(\"C:\\\\Users\\\\1\\\\Desktop\\\\ItPLanetProject2\\\\SavedModels\\\\first_model.keras\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
